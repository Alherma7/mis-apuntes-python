<nav>
  <h2>📚 Índice de módulos</h2>
  <ul>
    <li><a href="#agrupacion">1. Agrupación para la exploración de conjuntos de datos</a></li>
    <li><a href="#jerarquica">2. Visualización con agrupación jerárquica y t-SNE</a></li>
    <li><a href="#pca">3. Decorrelación de tus datos y reducción de dimensiones</a></li>
    <li><a href="#interpretables">4. Descubrir rasgos interpretables</a></li>
  </ul>
  <hr>
</nav>

<section id="agrupacion">
  <h2>📍 Visualización inicial de agrupaciones</h2>
  <pre><code>xs = points[:, 0]
ys = points[:, 1]

plt.scatter(xs, ys)
plt.show()</code></pre>
</section>

<section>
  <h2>🔢 Aplicación de KMeans</h2>
  <pre><code>from sklearn.cluster import KMeans

model = KMeans(n_clusters=3)
model.fit(points)
labels = model.predict(new_points)

print(labels)</code></pre>
</section>

<section>
  <h2>📌 Visualización de etiquetas y centroides</h2>
  <pre><code>plt.scatter(new_points[:, 0], new_points[:, 1], c=labels, alpha=0.5)

centroids = model.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker='D', s=50)
plt.show()</code></pre>
</section>

<section>
  <h2>📈 Determinar número óptimo de clústeres</h2>
  <pre><code>ks = range(1, 6)
inertias = []

for k in ks:
    model = KMeans(n_clusters=k)
    model.fit(samples)
    inertias.append(model.inertia_)

plt.plot(ks, inertias, '-o')
plt.xlabel('Número de clusters, k')
plt.ylabel('Inercia')
plt.xticks(ks)
plt.show()</code></pre>
</section>

<section>
  <h2>📊 Evaluación con tabulación cruzada</h2>
  <pre><code>labels = model.fit_predict(samples)

import pandas as pd
df = pd.DataFrame({'labels': labels, 'varieties': varieties})
ct = pd.crosstab(df['labels'], df['varieties'])

print(ct)</code></pre>
</section>

<section>
  <h2>⚙️ Pipeline para datos de peces</h2>
  <pre><code>from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
kmeans = KMeans(n_clusters=4)
pipeline = make_pipeline(scaler, kmeans)
labels = pipeline.fit_predict(samples)</code></pre>
</section>

<section>
  <h2>🐟 Tabulación cruzada en peces</h2>
  <pre><code>df = pd.DataFrame({'labels': labels, 'species': species})
ct = pd.crosstab(df['labels'], df['species'])
print(ct)</code></pre>
</section>

<section>
  <h2>🏢 Agrupación de acciones con Normalizer</h2>
  <pre><code>from sklearn.preprocessing import Normalizer

normalizer = Normalizer()
kmeans = KMeans(n_clusters=10)
pipeline = make_pipeline(normalizer, kmeans)
pipeline.fit(movements)</code></pre>
</section>

<section>
  <h2>📋 Visualizar empresas agrupadas</h2>
  <pre><code>labels = pipeline.predict(movements)

df = pd.DataFrame({'Labels': labels, 'companies': companies})
print(df.sort_values(by='Labels'))</code></pre>
</section>

<section id="jerarquica">
  <h2>🔗 Agrupación jerárquica de datos de granos</h2>
  <pre><code>from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

mergings = linkage(samples, method='complete')

dendrogram(mergings,
           labels=varieties,
           leaf_rotation=90,
           leaf_font_size=6)
plt.show()</code></pre>
</section>

<section>
  <h2>🏛️ Agrupación jerárquica de empresas</h2>
  <pre><code>from sklearn.preprocessing import normalize

normalized_movements = normalize(movements)
mergings = linkage(normalized_movements, method='complete')

dendrogram(mergings,
           labels=companies,
           leaf_rotation=90,
           leaf_font_size=6)
plt.show()</code></pre>
</section>

<section>
  <h2>🕊️ Agrupación jerárquica con método 'single'</h2>
  <pre><code>mergings = linkage(samples, method='single')

dendrogram(mergings,
           labels=country_names,
           leaf_rotation=90,
           leaf_font_size=6)
plt.show()</code></pre>
</section>

<section>
  <h2>🔖 Extraer etiquetas de clústeres</h2>
  <pre><code>from scipy.cluster.hierarchy import fcluster
import pandas as pd

labels = fcluster(mergings, t=6, criterion='distance')

df = pd.DataFrame({'labels': labels, 'varieties': varieties})
ct = pd.crosstab(df['labels'], df['varieties'])

print(ct)</code></pre>
</section>

<section>
  <h2>🌀 Visualización t-SNE de datos de granos</h2>
  <pre><code>from sklearn.manifold import TSNE

model = TSNE(learning_rate=200)
tsne_features = model.fit_transform(samples)

xs = tsne_features[:, 0]
ys = tsne_features[:, 1]

plt.scatter(xs, ys, c=variety_numbers)
plt.show()</code></pre>
</section>

<section>
  <h2>💹 Mapa t-SNE de mercado financiero</h2>
  <pre><code>model = TSNE(learning_rate=50)
tsne_features = model.fit_transform(normalized_movements)

xs = tsne_features[:, 0]
ys = tsne_features[:, 1]

plt.scatter(xs, ys, alpha=0.5)

for x, y, name in zip(xs, ys, companies):
    plt.annotate(name, (x, y), fontsize=5, alpha=0.75)

plt.show()</code></pre>
</section>
<section>
  <h2>📉 Correlación de medidas con Pearson</h2>
  <pre><code>import matplotlib.pyplot as plt
from scipy.stats import pearsonr

width = grains[:, 0]
length = grains[:, 1]

plt.scatter(width, length)
plt.axis('equal')
plt.show()

correlation, _ = pearsonr(width, length)
print("Correlación:", correlation)</code></pre>
</section>

<section id="pca">
  <h2>🔄 Decorrelación con PCA</h2>
  <pre><code>from sklearn.decomposition import PCA

model = PCA()
pca_features = model.fit_transform(grains)

xs = pca_features[:, 0]
ys = pca_features[:, 1]

plt.scatter(xs, ys)
plt.axis('equal')
plt.show()

correlation, _ = pearsonr(xs, ys)
print("Correlación decorrelada:", correlation)</code></pre>
</section>

<section>
  <h2>📏 Primer componente principal</h2>
  <pre><code>plt.scatter(grains[:, 0], grains[:, 1])
model = PCA()
model.fit(grains)

mean = model.mean_
first_pc = model.components_[0, :]

plt.arrow(mean[0], mean[1],
          first_pc[0], first_pc[1],
          color='red', width=0.01)

plt.axis('equal')
plt.show()</code></pre>
</section>

<section>
  <h2>📊 Varianza explicada por características PCA</h2>
  <pre><code>from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

scaler = StandardScaler()
pca = PCA()
pipeline = make_pipeline(scaler, pca)

pipeline.fit(samples)

features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('Feature PCA')
plt.ylabel('Varianza')
plt.xticks(features)
plt.show()</code></pre>
</section>

<section>
  <h2>🧪 Reducción dimensional a 2 componentes</h2>
  <pre><code>pca = PCA(n_components=2)
pca.fit(scaled_samples)
pca_features = pca.transform(scaled_samples)

print(pca_features.shape)</code></pre>
</section>
<section id="interpretables">
  <h2>🧠 NMF aplicado a artículos de Wikipedia</h2>
  <pre><code>from sklearn.decomposition import NMF

model = NMF(n_components=6)
nmf_features = model.fit_transform(articles)

print(nmf_features.round(2))</code></pre>
</section>

<section>
  <h2>🔍 Características NMF por artículo</h2>
  <pre><code>import pandas as pd

df = pd.DataFrame(nmf_features, index=titles)
print(df.loc['Anne Hathaway'])
print(df.loc['Denzel Washington'])</code></pre>
</section>

<section>
  <h2>📚 Interpretar temas NMF</h2>
  <pre><code>components_df = pd.DataFrame(model.components_, columns=words)
print(components_df.shape)

component = components_df.iloc[3, :]
print(component.nlargest())</code></pre>
</section>

<section>
  <h2>💡 Matriz tf-idf para documentos</h2>
  <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
csr_mat = tfidf.fit_transform(documents)

print(csr_mat.toarray())
words = tfidf.get_feature_names_out()
print(words)</code></pre>
</section>

<section>
  <h2>🖼️ Visualización de imágenes LED</h2>
  <pre><code>digit = samples[0, :]
bitmap = digit.reshape((13, 8))

plt.imshow(bitmap, cmap='gray', interpolation='nearest')
plt.colorbar()
plt.show()</code></pre>
</section>

<section>
  <h2>⚙️ NMF sobre imágenes LED</h2>
  <pre><code>model = NMF(n_components=7)
features = model.fit_transform(samples)

for component in model.components_:
    show_as_image(component)

digit_features = features[0]
print(digit_features)</code></pre>
</section>

<section>
  <h2>⛔ PCA sobre imágenes LED</h2>
  <pre><code>from sklearn.decomposition import PCA

model = PCA(n_components=7)
features = model.fit_transform(samples)

for component in model.components_:
    show_as_image(component)</code></pre>
</section>

<section>
  <h2>⚽ Recomendación por similitud: Cristiano Ronaldo</h2>
  <pre><code>from sklearn.preprocessing import normalize
import pandas as pd

norm_features = normalize(nmf_features)
df = pd.DataFrame(norm_features, index=titles)

article = df.loc['Cristiano Ronaldo']
similarities = df.dot(article)

print(similarities.nlargest())</code></pre>
</section>

<section>
  <h2>🎵 Recomendación de artistas musicales</h2>
  <pre><code>from sklearn.decomposition import NMF
from sklearn.preprocessing import Normalizer, MaxAbsScaler
from sklearn.pipeline import make_pipeline
import pandas as pd

scaler = MaxAbsScaler()
nmf = NMF(n_components=20)
normalizer = Normalizer()
pipeline = make_pipeline(scaler, nmf, normalizer)

norm_features = pipeline.fit_transform(artists)
df = pd.DataFrame(norm_features, index=artist_names)

artist = df.loc['Bruce Springsteen']
similarities = df.dot(artist)

print(similarities.nlargest())</code></pre>
</section>
